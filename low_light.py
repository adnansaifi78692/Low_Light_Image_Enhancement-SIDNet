{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Train Script for See-In-The-Dark (SID) Low-Light Enhancement\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# Set deterministic settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# --------------------------------------------------\n",
    "class Config:\n",
    "    device         = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    data_dir       = \"C:/Users/ADNAN SAIFI/Desktop/LOLv2/Real_captured/Train\"\n",
    "    ckpt_dir       = \"ckpt_color3\"\n",
    "    out_dir        = \"samples_color3\"\n",
    "    batch_size     = 4\n",
    "    patch_size     = 256\n",
    "    num_workers    = 0\n",
    "    lr             = 3e-4\n",
    "    n_epochs       = 100\n",
    "    sample_every   = 5\n",
    "    resume_ckpt    = None  # Set to None to start from scratch\n",
    "\n",
    "os.makedirs(Config.ckpt_dir, exist_ok=True)\n",
    "os.makedirs(Config.out_dir, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. DATASET\n",
    "# --------------------------------------------------\n",
    "class LowLightDataset(Dataset):\n",
    "    def __init__(self, low_dir, normal_dir, patch_size, train=True):\n",
    "        super().__init__()\n",
    "        self.low_images    = sorted(os.listdir(low_dir))\n",
    "        self.normal_images = sorted(os.listdir(normal_dir))\n",
    "        self.low_dir       = low_dir\n",
    "        self.normal_dir    = normal_dir\n",
    "        self.ps            = patch_size\n",
    "        self.train         = train\n",
    "        self.tf            = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low  = Image.open(os.path.join(self.low_dir,    self.low_images[idx])).convert('RGB')\n",
    "        norm = Image.open(os.path.join(self.normal_dir, self.normal_images[idx])).convert('RGB')\n",
    "\n",
    "        if low.size[0] < self.ps or low.size[1] < self.ps:\n",
    "            low  = low.resize((self.ps, self.ps), Image.BICUBIC)\n",
    "            norm = norm.resize((self.ps, self.ps), Image.BICUBIC)\n",
    "\n",
    "        if self.train:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(low, output_size=(self.ps, self.ps))\n",
    "            low  = transforms.functional.crop(low, i, j, h, w)\n",
    "            norm = transforms.functional.crop(norm, i, j, h, w)\n",
    "\n",
    "        return self.tf(low), self.tf(norm)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. SID NETWORK\n",
    "# --------------------------------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch, 3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch, ch, 3, padding=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "class SIDNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(3, 32, 3, padding=1, bias=False)\n",
    "        self.down1 = nn.Conv2d(32, 64, 4, stride=2, padding=1, bias=False)\n",
    "        self.rb1 = ResidualBlock(64)\n",
    "        self.down2 = nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False)\n",
    "        self.rb2 = ResidualBlock(128)\n",
    "        self.rb_mid = ResidualBlock(128)\n",
    "        self.rb_mid2 = ResidualBlock(128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False)\n",
    "        self.rb3 = ResidualBlock(64)\n",
    "        self.up2 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1, bias=False)\n",
    "        self.rb4 = ResidualBlock(32)\n",
    "        self.conv_out = nn.Conv2d(32, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv_in(x)\n",
    "        x1 = F.relu(self.rb1(self.down1(x0)))\n",
    "        x2 = F.relu(self.rb2(self.down2(x1)))\n",
    "        xm = self.rb_mid(self.rb_mid2(x2))\n",
    "        y1 = F.relu(self.rb3(self.up1(xm))) + x1\n",
    "        y2 = F.relu(self.rb4(self.up2(y1))) + x0\n",
    "        return torch.sigmoid(self.conv_out(y2))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. LOSS\n",
    "# --------------------------------------------------\n",
    "class ColorConstancyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorConstancyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        mean_pred = torch.mean(pred, dim=(1, 2, 3), keepdim=True)\n",
    "        mean_gt = torch.mean(gt, dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "        # Color constancy loss to match the mean color\n",
    "        return F.mse_loss(mean_pred, mean_gt)    \n",
    "\n",
    "def loss_fn(pred, gt):\n",
    "    l1 = F.l1_loss(pred, gt)\n",
    "    ms = 1 - ssim(pred, gt, data_range=1.0, size_average=True)\n",
    "    return l1 + ms\n",
    "\n",
    "def loss_fn2(pred, gt):\n",
    "    l2 = F.mse_loss(pred, gt)\n",
    "    ms = 1 - ssim(pred, gt, data_range=1.0, size_average=True)\n",
    "    return l2 + ms\n",
    "\n",
    "def exposure_loss(pred, patch_size=16):\n",
    "    gray = 0.299 * pred[:,0,:,:] + 0.587 * pred[:,1,:,:] + 0.114 * pred[:,2,:,:]\n",
    "    mean = F.avg_pool2d(gray.unsqueeze(1), patch_size)\n",
    "    return torch.mean((mean - 0.6) ** 2)\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg16(pretrained=True).features[:16]\n",
    "        self.vgg = vgg.eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, pred, gt):\n",
    "        pred = TF.resize(pred, (224, 224))\n",
    "        gt = TF.resize(gt, (224, 224))\n",
    "        return F.l1_loss(self.vgg(pred), self.vgg(gt))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. SAVE SAMPLE IMAGE\n",
    "# --------------------------------------------------\n",
    "def save_sample(model, low, norm, epoch):\n",
    "    model.eval()\n",
    "    low = low.to(Config.device)\n",
    "    norm = norm.to(Config.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(low)\n",
    "    combo = torch.cat([low, norm, out], dim=3)\n",
    "    save_image(combo, f\"{Config.out_dir}/epoch_{epoch}.png\", normalize=True)\n",
    "    model.train()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. TRAINING LOOP\n",
    "# --------------------------------------------------\n",
    "def train():\n",
    "    print(f\"Using device: {Config.device}\")\n",
    "    \n",
    "    full_dataset = LowLightDataset(\n",
    "        low_dir=os.path.join(Config.data_dir, \"Low\"),\n",
    "        normal_dir=os.path.join(Config.data_dir, \"Normal\"),\n",
    "        patch_size=Config.patch_size,\n",
    "        train=True\n",
    "    )\n",
    "\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=Config.num_workers, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=Config.num_workers, drop_last=False)\n",
    "\n",
    "    net = SIDNet().to(Config.device)\n",
    "    opt = optim.Adam(net.parameters(), lr=Config.lr, betas=(0.9, 0.999))\n",
    "    vgg_loss_fn = VGGPerceptualLoss().to(Config.device)\n",
    "    color_loss_fn = ColorConstancyLoss().to(Config.device)\n",
    "\n",
    "    start_epoch = 1\n",
    "\n",
    "    if Config.resume_ckpt is None:\n",
    "        ckpt_files = [f for f in os.listdir(Config.ckpt_dir) if f.endswith(\".pth\")]\n",
    "        if ckpt_files:\n",
    "            latest_ckpt = max(ckpt_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "            Config.resume_ckpt = os.path.join(Config.ckpt_dir, latest_ckpt)\n",
    "\n",
    "    if Config.resume_ckpt is not None and os.path.exists(Config.resume_ckpt):\n",
    "        print(f\"Resuming training from {Config.resume_ckpt}\")\n",
    "        ckpt = torch.load(Config.resume_ckpt, map_location=Config.device)\n",
    "        net.load_state_dict(ckpt['model'])\n",
    "        opt.load_state_dict(ckpt['optimizer'])\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "    else:\n",
    "        print(\"Training from scratch.\")\n",
    "\n",
    "    for ep in range(start_epoch, Config.n_epochs + 1):\n",
    "        net.train()\n",
    "        tot_loss = 0\n",
    "        tot_loss1 = 0\n",
    "        tot_loss2 = 0\n",
    "        tot_exp_loss = 0\n",
    "        tot_color_loss = 0\n",
    "        tot_vgg_loss = 0\n",
    "\n",
    "        for low, norm in tqdm(train_loader, desc=f\"[Train] Epoch {ep}/{Config.n_epochs}\"):\n",
    "            low, norm = low.to(Config.device), norm.to(Config.device)\n",
    "            out = net(low)\n",
    "\n",
    "            loss1 = loss_fn(out, norm)\n",
    "            loss2 = loss_fn2(out, norm)\n",
    "            exp_loss = exposure_loss(out)\n",
    "            col_loss = color_loss_fn(out, norm)\n",
    "            vgg_feat_loss = vgg_loss_fn(out, norm)\n",
    "\n",
    "            loss = (\n",
    "                1.0 * loss1 +\n",
    "                1.0 * loss2 +\n",
    "                0.3 * exp_loss +\n",
    "                0.5 * col_loss +\n",
    "                0.1 * vgg_feat_loss\n",
    "            )\n",
    "\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item()\n",
    "            tot_loss1 += loss1.item()\n",
    "            tot_loss2 += loss2.item()\n",
    "            tot_exp_loss += exp_loss.item()\n",
    "            tot_color_loss += col_loss.item()\n",
    "            tot_vgg_loss += vgg_feat_loss.item()\n",
    "\n",
    "        avg_train_loss = tot_loss / len(train_loader)\n",
    "        print(f\"▶ Epoch {ep} | Train Loss: {avg_train_loss:.6f} | L1+SSIM: {tot_loss1/len(train_loader):.4f} | MSE+SSIM: {tot_loss2/len(train_loader):.4f} | EXP: {tot_exp_loss/len(train_loader):.4f} | COLOR: {tot_color_loss/len(train_loader):.4f} | VGG: {tot_vgg_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        val_loss1 = 0\n",
    "        val_loss2 = 0\n",
    "        val_exp_loss = 0\n",
    "        val_color_loss = 0\n",
    "        val_vgg_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for low, norm in tqdm(val_loader, desc=f\"[Val]   Epoch {ep}/{Config.n_epochs}\"):\n",
    "                low, norm = low.to(Config.device), norm.to(Config.device)\n",
    "                out = net(low)\n",
    "\n",
    "                loss1 = loss_fn(out, norm)\n",
    "                loss2 = loss_fn2(out, norm)\n",
    "                exp_loss = exposure_loss(out)\n",
    "                col_loss = color_loss_fn(out, norm)\n",
    "                vgg_feat_loss = vgg_loss_fn(out, norm)\n",
    "\n",
    "                loss = (\n",
    "                    1.0 * loss1 +\n",
    "                    1.0 * loss2 +\n",
    "                    0.3 * exp_loss +\n",
    "                    0.5 * col_loss +\n",
    "                    0.1 * vgg_feat_loss\n",
    "                )\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_loss1 += loss1.item()\n",
    "                val_loss2 += loss2.item()\n",
    "                val_exp_loss += exp_loss.item()\n",
    "                val_color_loss += col_loss.item()\n",
    "                val_vgg_loss += vgg_feat_loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"▶ Epoch {ep} | Val Loss: {avg_val_loss:.6f} | L1+SSIM: {val_loss1/len(val_loader):.4f} | MSE+SSIM: {val_loss2/len(val_loader):.4f} | EXP: {val_exp_loss/len(val_loader):.4f} | COLOR: {val_color_loss/len(val_loader):.4f} | VGG: {val_vgg_loss/len(val_loader):.4f}\")\n",
    "\n",
    "        save_sample(net, low, norm, ep)\n",
    "        ckpt = {\n",
    "            'model': net.state_dict(),\n",
    "            'optimizer': opt.state_dict(),\n",
    "            'epoch': ep\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(Config.ckpt_dir, f\"sid_epoch_{ep}.pth\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. MAIN\n",
    "# --------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
